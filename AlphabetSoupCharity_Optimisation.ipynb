{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Optimise the model\n",
    "Using your knowledge of TensorFlow, optimise your model to achieve a target predictive accuracy higher than 75%.\n",
    "Using any or all of the following methods to optimise your model:\n",
    "- Adjust the input data to ensure that no variables or outliers are causing confusion in the model, such as:\n",
    "    - Dropping more or fewer columns.\n",
    "    - Creating more bins for rare occurrences in columns.\n",
    "    - Increasing or decreasing the number of values for each bin.\n",
    "- Add more neurons to a hidden layer.\n",
    "- Add more hidden layers.\n",
    "- Use different activation functions for the hidden layers.\n",
    "- Add or reduce the number of epochs to the training regimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3A: Preprocess the Data\n",
    "Preprocess the dataset like you did in Step 1, Be sure to adjust for any modifications that came out of optimising the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN                          NAME APPLICATION_TYPE  AFFILIATION  \\\n",
       "0  10520599  BLUE KNIGHTS MOTORCYCLE CLUB              T10  Independent   \n",
       "\n",
       "  CLASSIFICATION    USE_CASE ORGANIZATION  STATUS INCOME_AMT  \\\n",
       "0          C1000  ProductDev  Association       1          0   \n",
       "\n",
       "  SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0                      N     5000              1  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into a Pandas DataFrame\n",
    "charity_df = pd.read_csv(\"Resources/charity_data.csv\")\n",
    "charity_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added outlier reduction before binning at attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_r/33n0_r6j30zdld79w7tl1h6c0000gn/T/ipykernel_35671/3705079654.py:2: FutureWarning: The default value of numeric_only in DataFrame.quantile is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  Q1 = charity_df.quantile(0.25)\n",
      "/var/folders/_r/33n0_r6j30zdld79w7tl1h6c0000gn/T/ipykernel_35671/3705079654.py:3: FutureWarning: The default value of numeric_only in DataFrame.quantile is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  Q3 = charity_df.quantile(0.75)\n",
      "/var/folders/_r/33n0_r6j30zdld79w7tl1h6c0000gn/T/ipykernel_35671/3705079654.py:10: FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version. Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`\n",
      "  outliers = ((charity_df < (Q1 - outlier_threshold)) | (charity_df > (Q3 + outlier_threshold))).any(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the IQR for each feature\n",
    "Q1 = charity_df.quantile(0.25)\n",
    "Q3 = charity_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the outlier threshold as 1.5 times the IQR above Q3 or below Q1\n",
    "outlier_threshold = 1.5 * IQR\n",
    "\n",
    "# Find the indices of the outliers\n",
    "outliers = ((charity_df < (Q1 - outlier_threshold)) | (charity_df > (Q3 + outlier_threshold))).any(axis=1)\n",
    "\n",
    "# Remove the outliers from the data\n",
    "charity_df = charity_df[~outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EIN                       26088\n",
       "NAME                      13247\n",
       "APPLICATION_TYPE             12\n",
       "AFFILIATION                   6\n",
       "CLASSIFICATION               65\n",
       "USE_CASE                      5\n",
       "ORGANIZATION                  4\n",
       "STATUS                        1\n",
       "INCOME_AMT                    9\n",
       "SPECIAL_CONSIDERATIONS        2\n",
       "ASK_AMT                     656\n",
       "IS_SUCCESSFUL                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "charity_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3     20081\n",
       "T4      1331\n",
       "T19      999\n",
       "T5       894\n",
       "T6       882\n",
       "T8       669\n",
       "T7       633\n",
       "T10      508\n",
       "T13       54\n",
       "T9        18\n",
       "T12       13\n",
       "T2         6\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "bin1 = charity_df['APPLICATION_TYPE'].value_counts()\n",
    "bin1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3       20081\n",
       "T4        1331\n",
       "T19        999\n",
       "T5         894\n",
       "T6         882\n",
       "T8         669\n",
       "T7         633\n",
       "T10        508\n",
       "Other       91\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "application_types_to_replace = list(bin1[bin1<100].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    charity_df['APPLICATION_TYPE'] = charity_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "charity_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    12554\n",
       "C2000     4692\n",
       "C1200     3997\n",
       "C2100     1622\n",
       "C3000     1537\n",
       "         ...  \n",
       "C1732        1\n",
       "C1728        1\n",
       "C4120        1\n",
       "C1245        1\n",
       "C2150        1\n",
       "Name: CLASSIFICATION, Length: 65, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "bin2 = charity_df['CLASSIFICATION'].value_counts()\n",
    "bin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    12554\n",
       "C2000     4692\n",
       "C1200     3997\n",
       "C2100     1622\n",
       "C3000     1537\n",
       "C7000      546\n",
       "C1700      223\n",
       "C4000      129\n",
       "C5000      106\n",
       "C1270       89\n",
       "C2700       75\n",
       "C7100       62\n",
       "C2800       61\n",
       "C1280       46\n",
       "C1300       42\n",
       "C1230       35\n",
       "C2300       28\n",
       "C1240       28\n",
       "C1400       27\n",
       "C7200       20\n",
       "C6000       14\n",
       "C8000       13\n",
       "C1250       13\n",
       "C7120       11\n",
       "C1278       10\n",
       "C1237        9\n",
       "C8200        9\n",
       "C1238        9\n",
       "C1235        9\n",
       "C1500        7\n",
       "C1720        6\n",
       "C1257        5\n",
       "C7210        5\n",
       "C2400        4\n",
       "C1600        4\n",
       "C4100        4\n",
       "C1260        3\n",
       "C1800        3\n",
       "C1267        2\n",
       "C1246        2\n",
       "C1256        2\n",
       "C0           2\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
    "bin2[bin2>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    12554\n",
       "C2000     4692\n",
       "C1200     3997\n",
       "C2100     1622\n",
       "C3000     1537\n",
       "C7000      546\n",
       "C1700      223\n",
       "C4000      129\n",
       "Other      108\n",
       "C5000      106\n",
       "C1270       89\n",
       "C2700       75\n",
       "C7100       62\n",
       "C2800       61\n",
       "C1280       46\n",
       "C1300       42\n",
       "C1230       35\n",
       "C2300       28\n",
       "C1240       28\n",
       "C1400       27\n",
       "C7200       20\n",
       "C6000       14\n",
       "C1250       13\n",
       "C8000       13\n",
       "C7120       11\n",
       "C1278       10\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "classifications_to_replace = list(bin2[bin2<10].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    charity_df['CLASSIFICATION'] = charity_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "charity_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PARENT BOOSTER USA INC                                    1130\n",
       "TOPS CLUB INC                                              765\n",
       "UNITED STATES BOWLING CONGRESS INC                         618\n",
       "WASHINGTON STATE UNIVERSITY                                487\n",
       "AMATEUR ATHLETIC UNION OF THE UNITED STATES INC            385\n",
       "                                                          ... \n",
       "SOUTHERN ARIZONA LUTHERAN CAMPING ASSOCIATION                1\n",
       "WIS TEQ NEEMIT                                               1\n",
       "YOUNG ARTISTS SYMPHONY ORCHESTRA                             1\n",
       "CATHOLIC CEMETERY AND CHARITABLE IRRV TR                     1\n",
       "AMERICAN FEDERATION OF GOVERNMENT EMPLOYEES LOCAL 2886       1\n",
       "Name: NAME, Length: 13247, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at NAME value counts for binning\n",
    "bin3 = charity_df['NAME'].value_counts()\n",
    "bin3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other                                                                    16923\n",
       "PARENT BOOSTER USA INC                                                    1130\n",
       "TOPS CLUB INC                                                              765\n",
       "UNITED STATES BOWLING CONGRESS INC                                         618\n",
       "WASHINGTON STATE UNIVERSITY                                                487\n",
       "                                                                         ...  \n",
       "VETERANS OF FOREIGN WARS OF THE U S AUXILIARY DEPARTMENT OF LOUISIANA       41\n",
       "AMERICAN YOUTH FOOTBALL INC                                                 41\n",
       "MUSIC TEACHERS NATIONAL ASSOCIATION INC                                     41\n",
       "MODERN QUILT GUILD INC                                                      40\n",
       "VETERANS OF FOREIGN WARS OF THE UNITED STATES AUX DEPT OF COLORADO          40\n",
       "Name: NAME, Length: 61, dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of names to be replaced\n",
    "# use the variable name `names_to_replace`\n",
    "names_to_replace = list(bin3[bin3<40].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for name in names_to_replace:\n",
    "    charity_df['NAME'] = charity_df['NAME'].replace(name,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "charity_df['NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "      <th>NAME_AIR FORCE ASSOCIATION</th>\n",
       "      <th>NAME_ALABAMA FEDERATION OF WOMENS CLUBS</th>\n",
       "      <th>NAME_ALPHA PHI SIGMA</th>\n",
       "      <th>NAME_AMATEUR ATHLETIC UNION OF THE UNITED STATES</th>\n",
       "      <th>NAME_AMATEUR ATHLETIC UNION OF THE UNITED STATES INC</th>\n",
       "      <th>NAME_AMERICAN ASSOCIATION OF UNIVERSITY WOMEN</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10556855</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10571689</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN  STATUS  ASK_AMT  IS_SUCCESSFUL  NAME_AIR FORCE ASSOCIATION  \\\n",
       "0  10520599       1     5000              1                         0.0   \n",
       "2  10547893       1     5000              0                         0.0   \n",
       "3  10553066       1     6692              1                         0.0   \n",
       "5  10556855       1     5000              1                         0.0   \n",
       "9  10571689       1     5000              0                         0.0   \n",
       "\n",
       "   NAME_ALABAMA FEDERATION OF WOMENS CLUBS  NAME_ALPHA PHI SIGMA  \\\n",
       "0                                      0.0                   0.0   \n",
       "2                                      0.0                   0.0   \n",
       "3                                      0.0                   0.0   \n",
       "5                                      0.0                   0.0   \n",
       "9                                      0.0                   0.0   \n",
       "\n",
       "   NAME_AMATEUR ATHLETIC UNION OF THE UNITED STATES  \\\n",
       "0                                               0.0   \n",
       "2                                               0.0   \n",
       "3                                               0.0   \n",
       "5                                               0.0   \n",
       "9                                               0.0   \n",
       "\n",
       "   NAME_AMATEUR ATHLETIC UNION OF THE UNITED STATES INC  \\\n",
       "0                                                0.0      \n",
       "2                                                0.0      \n",
       "3                                                0.0      \n",
       "5                                                0.0      \n",
       "9                                                0.0      \n",
       "\n",
       "   NAME_AMERICAN ASSOCIATION OF UNIVERSITY WOMEN  ...  INCOME_AMT_1-9999  \\\n",
       "0                                            0.0  ...                0.0   \n",
       "2                                            0.0  ...                0.0   \n",
       "3                                            0.0  ...                0.0   \n",
       "5                                            0.0  ...                0.0   \n",
       "9                                            0.0  ...                0.0   \n",
       "\n",
       "   INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  \\\n",
       "0                     0.0                       0.0                 0.0   \n",
       "2                     0.0                       0.0                 0.0   \n",
       "3                     1.0                       0.0                 0.0   \n",
       "5                     0.0                       0.0                 0.0   \n",
       "9                     0.0                       0.0                 0.0   \n",
       "\n",
       "   INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  INCOME_AMT_50M+  \\\n",
       "0               0.0                     0.0              0.0   \n",
       "2               0.0                     0.0              0.0   \n",
       "3               0.0                     0.0              0.0   \n",
       "5               0.0                     0.0              0.0   \n",
       "9               0.0                     0.0              0.0   \n",
       "\n",
       "   INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \n",
       "0                0.0                       1.0                       0.0  \n",
       "2                0.0                       1.0                       0.0  \n",
       "3                0.0                       1.0                       0.0  \n",
       "5                0.0                       1.0                       0.0  \n",
       "9                0.0                       1.0                       0.0  \n",
       "\n",
       "[5 rows x 126 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "charity_df = pd.get_dummies(charity_df,dtype=float)\n",
    "charity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "charity_df = charity_df.drop(columns=[\"EIN\"])\n",
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "X = charity_df.drop(columns=[\"IS_SUCCESSFUL\"])\n",
    "y = charity_df[\"IS_SUCCESSFUL\"]\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Scale the data using the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "# Convert y_train and y_test to categorical values\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26088, 125)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print charity_df shape\n",
    "charity_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3B: Optimise Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "## Script 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_144 (Dense)           (None, 80)                10000     \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 2)                 162       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,642\n",
      "Trainable params: 16,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "number_input_features = len( X_scaler[0])\n",
    "\n",
    "# neurons in the input layer\n",
    "input_layer_nodes = 80\n",
    "\n",
    "# Define the hidden layers\n",
    "hidden_nodes_layer_1 = 80\n",
    "\n",
    "# neurons in the output layer\n",
    "output_layer_nodes = 2\n",
    "\n",
    "# Design the neural network model\n",
    "number_input_features = len(X_train.columns)\n",
    "\n",
    "nn1 = tf.keras.models.Sequential()\n",
    "\n",
    "# Add the input layer\n",
    "nn1.add(Dense(units=input_layer_nodes, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn1.add(Dense(units=hidden_nodes_layer_1, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "nn1.add(Dense(units=output_layer_nodes, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.4657 - accuracy: 0.7756\n",
      "Epoch 2/5\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.4415 - accuracy: 0.7886\n",
      "Epoch 3/5\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.4372 - accuracy: 0.7906\n",
      "Epoch 4/5\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.4344 - accuracy: 0.7921\n",
      "Epoch 5/5\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.4326 - accuracy: 0.7940\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "nn1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model with 5 epochs\n",
    "model_1 = nn1.fit(X_scaler, y_train_categorical, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 - 0s - loss: 0.4440 - accuracy: 0.7856 - 311ms/epoch - 2ms/step\n",
      "Test Loss: 0.44402551651000977\n",
      "Test Accuracy: 0.7856485843658447\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "test_loss, test_accuracy = nn1.evaluate(X_test_scaler, y_test_categorical, verbose=2)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n204/204 - 0s - loss: 0.4481 - accuracy: 0.7850 - 356ms/epoch - 2ms/step\\nTest Loss: 0.44812464714050293\\nTest Accuracy: 0.785035252571106\\n'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt 1:\n",
    "\n",
    "'''\n",
    "268/268 - 0s - loss: 0.5492 - accuracy: 0.7313 - 377ms/epoch - 1ms/step\n",
    "Test Loss: 0.5492093563079834\n",
    "Test Accuracy: 0.7313119769096375\n",
    "'''\n",
    "\n",
    "# Attempt 2: Added outlier reduction before binning\n",
    "\n",
    "'''\n",
    "204/204 - 0s - loss: 0.8306 - accuracy: 0.7459 - 375ms/epoch - 2ms/step\n",
    "Test Loss: 0.8305676579475403\n",
    "Test Accuracy: 0.7459368109703064\n",
    "'''\n",
    "\n",
    "# Attempt 3: binning values in the \"NAME\" column\n",
    "\n",
    "'''\n",
    "204/204 - 0s - loss: 0.5579 - accuracy: 0.7453 - 394ms/epoch - 2ms/step\n",
    "Test Loss: 0.5578669309616089\n",
    "Test Accuracy: 0.7453235387802124\n",
    "'''\n",
    "\n",
    "# Attempt 4: nodes 80, 80, 2 + FIXED PREPROCESSING -> # Convert categorical data to numeric with `pd.get_dummies`\n",
    "\n",
    "'''\n",
    "204/204 - 0s - loss: 0.4481 - accuracy: 0.7850 - 356ms/epoch - 2ms/step\n",
    "Test Loss: 0.44812464714050293\n",
    "Test Accuracy: 0.785035252571106\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "# Script 2:\n",
    "Added Dropout layers to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_147 (Dense)           (None, 90)                11250     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 90)                0         \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 90)                8190      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 90)                0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 2)                 182       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,622\n",
      "Trainable params: 19,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "number_input_features = len( X_scaler[0])\n",
    "\n",
    "# neurons in the input layer\n",
    "input_layer_nodes = 90\n",
    "\n",
    "# Define the hidden layers\n",
    "hidden_nodes_layer_1 = 90\n",
    "\n",
    "# neurons in the output layer\n",
    "output_layer_nodes = 2\n",
    "\n",
    "# Design the neural network model\n",
    "number_input_features = len(X_train.columns)\n",
    "\n",
    "nn2 = tf.keras.models.Sequential()\n",
    "\n",
    "# Add the input layer\n",
    "nn2.add(Dense(units=input_layer_nodes, input_dim=number_input_features, activation=\"tanh\"))\n",
    "\n",
    "# Add a Dropout layer to prevent overfitting\n",
    "nn2.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn2.add(Dense(units=hidden_nodes_layer_1, activation=\"sigmoid\"))\n",
    "\n",
    "# Add another Dropout layer\n",
    "nn2.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Add the output layer \n",
    "nn2.add(Dense(units=output_layer_nodes, activation=\"softmax\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "612/612 [==============================] - 2s 2ms/step - loss: 0.4448 - accuracy: 0.7853\n",
      "Epoch 2/5\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.4434 - accuracy: 0.7873\n",
      "Epoch 3/5\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.4409 - accuracy: 0.7891\n",
      "Epoch 4/5\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.4381 - accuracy: 0.7892\n",
      "Epoch 5/5\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.4385 - accuracy: 0.7899\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "nn2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with 5 epochs\n",
    "model_2 = nn2.fit(X_scaler, y_train_categorical, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 - 1s - loss: 0.4423 - accuracy: 0.7861 - 503ms/epoch - 2ms/step\n",
      "Test Loss: 0.44234219193458557\n",
      "Test Accuracy: 0.7861085534095764\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "test_loss, test_accuracy = nn2.evaluate(X_test_scaler, y_test_categorical, verbose=2)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n204/204 - 0s - loss: 0.4471 - accuracy: 0.7778 - 368ms/epoch - 2ms/step\\nTest Loss: 0.44707298278808594\\nTest Accuracy: 0.7778288722038269\\n'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt 1: \n",
    "\n",
    "'''\n",
    "204/204 - 0s - loss: 0.5416 - accuracy: 0.7423 - 321ms/epoch - 2ms/step\n",
    "Test Loss: 0.5415589213371277\n",
    "Test Accuracy: 0.7422569990158081\n",
    "'''\n",
    "\n",
    "# Attempt 2: FIXED PREPROCESSING -> # Convert categorical data to numeric with `pd.get_dummies`\n",
    "\n",
    "'''\n",
    "204/204 - 0s - loss: 0.4471 - accuracy: 0.7778 - 368ms/epoch - 2ms/step\n",
    "Test Loss: 0.44707298278808594\n",
    "Test Accuracy: 0.7778288722038269\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "# Script 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_150 (Dense)           (None, 100)               12500     \n",
      "                                                                 \n",
      " dense_151 (Dense)           (None, 80)                8080      \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 2)                 162       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,222\n",
      "Trainable params: 27,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "number_input_features = len(X_scaler[0])\n",
    "\n",
    "# neurons in the input layer\n",
    "input_layer_nodes = 100\n",
    "\n",
    "# Define the hidden layers\n",
    "hidden_nodes_layer_1 = 80\n",
    "hidden_nodes_layer_2 = 80\n",
    "\n",
    "# neurons in the output layer\n",
    "output_layer_nodes = 2\n",
    "\n",
    "# Design the neural network model\n",
    "number_input_features = len(X_train.columns)\n",
    "\n",
    "nn3 = tf.keras.models.Sequential()\n",
    "\n",
    "# Add the input layer with \"relu\" activation function relu -> tanh -> sigmoid\n",
    "nn3.add(Dense(units=input_layer_nodes, input_dim=number_input_features, activation=\"relu\", kernel_regularizer=l2(0.02)))\n",
    "\n",
    "# Add the first hidden layer with \"relu\" activation function relu -> tanh -> sigmoid\n",
    "nn3.add(Dense(units=hidden_nodes_layer_1, activation=\"relu\", kernel_regularizer=l2(0.02)))\n",
    "\n",
    "# Add the second hidden layer with \"relu\" activation function relu -> tanh -> sigmoid\n",
    "nn3.add(Dense(units=hidden_nodes_layer_2, activation=\"sigmoid\", kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Add the output layer with \"softmax\" activation function # softmax -> sigmoid -> tanh -> sigmoid\n",
    "nn3.add(Dense(units=output_layer_nodes, activation=\"sigmoid\", kernel_regularizer=l2(0.02)))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "612/612 [==============================] - 2s 1ms/step - loss: 1.1057 - accuracy: 0.7649\n",
      "Epoch 2/10\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.6055 - accuracy: 0.7816\n",
      "Epoch 3/10\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.5963 - accuracy: 0.7826\n",
      "Epoch 4/10\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.5929 - accuracy: 0.7794\n",
      "Epoch 5/10\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.5919 - accuracy: 0.7801\n",
      "Epoch 6/10\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.5901 - accuracy: 0.7822\n",
      "Epoch 7/10\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.5894 - accuracy: 0.7813\n",
      "Epoch 8/10\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.5884 - accuracy: 0.7805\n",
      "Epoch 9/10\n",
      "612/612 [==============================] - 1s 1ms/step - loss: 0.5877 - accuracy: 0.7839\n",
      "Epoch 10/10\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 0.5895 - accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "nn3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with 10 epochs\n",
    "model_4 = nn3.fit(X_scaler, y_train_categorical, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 - 0s - loss: 0.5956 - accuracy: 0.7751 - 353ms/epoch - 2ms/step\n",
      "Test Loss: 0.5956299304962158\n",
      "Test Accuracy: 0.775068998336792\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "test_loss, test_accuracy = nn3.evaluate(X_test_scaler, y_test_categorical, verbose=2)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n204/204 - 0s - loss: 0.5943 - accuracy: 0.7715 - 354ms/epoch - 2ms/step\\nTest Loss: 0.5942786931991577\\nTest Accuracy: 0.771542489528656\\n'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt 1: nodes 80, 80, 80 , 2 \n",
    "\n",
    "'''\n",
    "204/204 - 0s - loss: 0.5749 - accuracy: 0.7380 - 355ms/epoch - 2ms/step\n",
    "Test Loss: 0.5749160051345825\n",
    "Test Accuracy: 0.7379637956619263\n",
    "'''\n",
    "\n",
    "# Attempt 2: nodes 100, 80, 80 , 2 \n",
    "\n",
    "'''\n",
    "204/204 - 0s - loss: 0.5749 - accuracy: 0.7380 - 299ms/epoch - 1ms/step\n",
    "Test Loss: 0.5749160051345825\n",
    "Test Accuracy: 0.7379637956619263\n",
    "'''\n",
    "\n",
    "# Attempt 3: nodes 100, 80, 80 , 2 + FIXED PREPROCESSING -> # Convert categorical data to numeric with `pd.get_dummies`\n",
    "\n",
    "'''\n",
    "204/204 - 0s - loss: 0.5943 - accuracy: 0.7715 - 354ms/epoch - 2ms/step\n",
    "Test Loss: 0.5942786931991577\n",
    "Test Accuracy: 0.771542489528656\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to an HDF5 file\n",
    "nn2.save(\"AlphabetSoupCharity_Optimisation.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write a Report on the Neural Network Model\n",
    "For this part of the assignment, you’ll write a report on the performance of the deep learning model you created for AlphabetSoup.\n",
    "The report should contain the following:\n",
    "\n",
    "\n",
    "Overview of the analysis: Explain the purpose of this analysis.\n",
    "\n",
    "Results: Using bulleted lists and images to support your answers, address the following questions.\n",
    "\n",
    "# Data Preprocessing\n",
    "- What variable(s) are the target(s) for your model?\n",
    "- What variable(s) are the features for your model?\n",
    "- What variable(s) should be removed from the input data because they are neither targets nor features?\n",
    "\n",
    "# Compiling, Training, and Evaluating the Model\n",
    "- How many neurons, layers, and activation functions did you select for your neural network model, and why?\n",
    "- Were you able to achieve the target model performance?\n",
    "- What steps did you take in your attempts to increase model performance?\n",
    "\n",
    "Summary: Summarise the overall results of the deep learning model. Include a recommendation for how a different model could solve this classification problem, and then explain your recommendation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "046794b2ecc5e4975d107f4bb8cebdb6afc929164a39b2843cc2cedc2138f45e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
